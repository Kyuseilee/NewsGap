"""
Google Gemini API é€‚é…å™¨
"""

import json
import asyncio
import logging
from typing import List, Optional
from datetime import datetime
import google.generativeai as genai

from models import Article, Analysis, AnalysisType, Trend, Signal, InformationGap
from llm.adapter import BaseLLMAdapter

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class GeminiAdapter(BaseLLMAdapter):
    """Google Gemini API é€‚é…å™¨ï¼ˆä½¿ç”¨å®˜æ–¹ Google GenAI SDKï¼‰"""
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: Optional[str] = None
    ):
        # ä½¿ç”¨æœ€æ–°ç¨³å®šçš„ Gemini 2.5 Flash æ¨¡å‹
        super().__init__(api_key=api_key, model=model or "gemini-2.5-flash")
        
        if not self.api_key:
            raise ValueError("Gemini API Key is required. Please configure it in Settings.")
        
        # é…ç½® Google GenAI
        genai.configure(api_key=self.api_key)
        
        # Gemini 2.5 Flashæ”¯æŒæœ€å¤š8192è¾“å‡ºtokens
        # è®¾ç½®candidate_count=1ç¡®ä¿ç”Ÿæˆå®Œæ•´å•ä¸€å€™é€‰
        self.client = genai.GenerativeModel(
            model_name=self.model,
            generation_config=genai.GenerationConfig(
                temperature=0.3,
                max_output_tokens=8192,  # Gemini 2.5 Flashçš„æœ€å¤§è¾“å‡ºtoken
                candidate_count=1,  # åªç”Ÿæˆ1ä¸ªå€™é€‰ï¼Œé¿å…åˆ†æ•£token
                # å¦‚æœä½¿ç”¨Gemini 2.5 Proï¼Œå¯ä»¥è®¾ç½®åˆ°æ›´é«˜
            )
        )
    
    async def analyze(
        self,
        articles: List[Article],
        analysis_type: AnalysisType,
        custom_prompt: Optional[str] = None
    ) -> Analysis:
        """ä½¿ç”¨ Gemini è¿›è¡Œåˆ†æ"""
        system_prompt = self._build_system_prompt(analysis_type)
        user_prompt = self._build_markdown_prompt(articles, custom_prompt)
        
        start_time = datetime.now()
        
        # ç»„åˆæç¤ºè¯
        full_prompt = f"{system_prompt}\n\n{user_prompt}"
        
        logger.info(f"å¼€å§‹ Gemini åˆ†æï¼Œæ–‡ç« æ•°é‡: {len(articles)}")
        logger.debug(f"æç¤ºè¯é•¿åº¦: {len(full_prompt)} å­—ç¬¦")
        
        try:
            # ä½¿ç”¨ asyncio.to_thread åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥è°ƒç”¨
            response = await asyncio.to_thread(
                self._sync_generate,
                full_prompt
            )
            
            response_text = response.text
            
            # è®°å½•å®Œæ•´å“åº”åˆ°æ—¥å¿—
            logger.info(f"Gemini å“åº”é•¿åº¦: {len(response_text)} å­—ç¬¦")
            logger.debug(f"Gemini å®Œæ•´å“åº”:\n{response_text}")
            
            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            try:
                with open('gemini_response.log', 'a', encoding='utf-8') as f:
                    f.write(f"\n{'='*80}\n")
                    f.write(f"æ—¶é—´: {datetime.now().isoformat()}\n")
                    f.write(f"æ¨¡å‹: {self.model}\n")
                    f.write(f"æ–‡ç« æ•°: {len(articles)}\n")
                    f.write(f"å“åº”é•¿åº¦: {len(response_text)}\n")
                    f.write(f"{'='*80}\n")
                    f.write(response_text)
                    f.write(f"\n{'='*80}\n\n")
                logger.info("å“åº”å·²å†™å…¥ gemini_response.log")
            except Exception as log_error:
                logger.warning(f"å†™å…¥æ—¥å¿—æ–‡ä»¶å¤±è´¥: {log_error}")
            
            # è·å– token ä½¿ç”¨ä¿¡æ¯
            token_usage = 0
            if hasattr(response, 'usage_metadata'):
                usage = response.usage_metadata
                if hasattr(usage, 'total_token_count'):
                    token_usage = usage.total_token_count
                    logger.info(f"Token ä½¿ç”¨: {token_usage}")
        
        except Exception as e:
            logger.error(f"Gemini API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            # å¦‚æœè°ƒç”¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
            return Analysis(
                analysis_type=analysis_type,
                article_ids=[a.id for a in articles if a.id],
                executive_brief=f"âŒ Gemini åˆ†æå¤±è´¥: {str(e)}",
                markdown_report=f"# åˆ†æå¤±è´¥\n\né”™è¯¯ä¿¡æ¯ï¼š{str(e)}",
                trends=[],
                signals=[],
                information_gaps=[],
                llm_backend="gemini",
                llm_model=self.model,
                token_usage=0,
                estimated_cost=0.0,
                processing_time_seconds=(datetime.now() - start_time).total_seconds()
            )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"åˆ†æå®Œæˆï¼Œè€—æ—¶: {processing_time:.2f}ç§’")
        
        # æå–æ‰§è¡Œæ‘˜è¦ï¼ˆå–ç¬¬ä¸€æ®µæˆ–å‰500å­—ï¼‰
        lines = response_text.strip().split('\n')
        executive_brief = ""
        for line in lines:
            if line.strip() and not line.strip().startswith('#'):
                executive_brief = line.strip()[:500]
                break
        
        if not executive_brief:
            executive_brief = response_text[:500]
        
        # è®¡ç®—æˆæœ¬
        cost_per_1k = self.get_model_info()['cost_per_1k_tokens']
        estimated_cost = (token_usage / 1000) * cost_per_1k
        
        # æ„å»º Analysis å¯¹è±¡
        return Analysis(
            analysis_type=analysis_type,
            article_ids=[a.id for a in articles if a.id],
            executive_brief=executive_brief,
            markdown_report=response_text,  # å®Œæ•´çš„ Markdown æŠ¥å‘Š
            trends=[],
            signals=[],
            information_gaps=[],
            llm_backend="gemini",
            llm_model=self.model,
            token_usage=token_usage,
            estimated_cost=estimated_cost,
            processing_time_seconds=processing_time
        )
    
    def _build_system_prompt(self, analysis_type: AnalysisType) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ - ä¿¡å·ä¼˜å…ˆç‰ˆ"""
        return """ä½ ä¸æ˜¯æ–°é—»æ‘˜è¦å™¨ï¼Œè€Œæ˜¯ä¸€å"å†³ç­–å¯¼å‘å‹è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆ"ã€‚

ä½ çš„ç›®æ ‡ä¸æ˜¯è¦†ç›–æ‰€æœ‰ä¿¡æ¯ï¼Œè€Œæ˜¯ï¼š
- åœ¨å¤§é‡æ‚è®¯ä¸­ï¼Œå¿«é€Ÿè¯†åˆ«**çœŸæ­£æ”¹å˜æ ¼å±€çš„å°‘æ•°ä¿¡å·**
- ä¸º"ç†æ€§å†³ç­–è€…"æä¾›**å¯è¡ŒåŠ¨ã€å¯å–èˆã€å¯å¿½ç•¥çš„ä¿¡æ¯ç»“æ„**

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. **æ®‹å¿ç­›é€‰åŸåˆ™**ï¼šå…è®¸å¿½ç•¥ã€åˆå¹¶ã€å¼±åŒ–å¤§é‡ä½ä»·å€¼æ–‡ç« ï¼Œåªæœ‰"æ”¹å˜åˆ¤æ–­"çš„å†…å®¹æ‰å€¼å¾—å±•å¼€
2. **ä¸»çº¿ä¼˜å…ˆåŸåˆ™**ï¼šå…ˆè¯†åˆ«3-5æ¡"ä»Šæ—¥ä¸»çº¿å™äº‹"ï¼Œæ‰€æœ‰æ–‡ç« åªæ˜¯è¿™äº›ä¸»çº¿çš„"è¯æ®"æˆ–"å™ªéŸ³"
3. **å»å‡å€¼åŸåˆ™**ï¼šé¿å…å¤§é‡7/10ã€8/10çš„æ¨¡ç³Šè¯„åˆ†ï¼Œé‡è¦æ€§å¿…é¡»å½¢æˆæ˜æ˜¾æ¢¯åº¦ï¼ˆ10/8/5/å¿½ç•¥ï¼‰
4. **å†³ç­–è§†è§’åŸåˆ™**ï¼šå‡è®¾è¯»è€…å…³æ³¨å®è§‚é£é™©ã€äº§ä¸šæ–¹å‘ä¸ä¸­é•¿æœŸé…ç½®çš„ç†æ€§å†³ç­–è€…
5. **å‹ç¼©ä¼˜å…ˆåŸåˆ™**ï¼šå®å¯å°‘å†™ä¸€åŠï¼Œä¹Ÿä¸è¦ä¿¡æ¯å¯†åº¦ä¸‹é™

ã€å…è®¸çš„æ“ä½œã€‘
- å°†åŒç±»æ–‡ç« åˆå¹¶ä¸º"ä¿¡æ¯ç°‡"
- å¯¹ä½ä»·å€¼å†…å®¹åªåšä¸€å¥è¯å¤„ç†ï¼Œç”šè‡³å®Œå…¨ä¸å†™
- æ˜ç¡®æŒ‡å‡ºï¼š"è¿™ä¸€ç±»ä¿¡æ¯ä»Šå¤©ä¸é‡è¦"

è¾“å‡ºè¦æ±‚ï¼š
1. ä½¿ç”¨æ¸…æ™°çš„ Markdown æ ¼å¼
2. ä¿¡æ¯å¯†åº¦ > è¦†ç›–ç‡
3. åˆ¤æ–­æ¸…æ™° > é¢é¢ä¿±åˆ°
4. ä¸è¦æˆªæ–­å†…å®¹ï¼Œç¡®ä¿æŠ¥å‘Šå®Œæ•´"""
    
    def _build_markdown_prompt(
        self,
        articles: List[Article],
        custom_prompt: Optional[str] = None
    ) -> str:
        """æ„å»º Markdown æŠ¥å‘Šæç¤ºè¯ - å‹ç¼©ç‰ˆ"""
        article_count = len(articles)
        
        # åŠ¨æ€è°ƒæ•´å‹ç¼©ç­–ç•¥
        if article_count <= 20:
            max_content = 1000
        elif article_count <= 50:
            max_content = 600
        elif article_count <= 100:
            max_content = 400
        else:
            max_content = 300
        
        # æ„å»ºæ–‡ç« åˆ—è¡¨ï¼ˆç´§å‡‘æ ¼å¼ï¼‰
        articles_text = f"# å¾…åˆ†æä¿¡æ¯æºï¼ˆå…± {article_count} æ¡ï¼‰\n\n"
        
        for i, article in enumerate(articles, 1):
            content = article.content[:max_content]
            if len(article.content) > max_content:
                content += "..."
            
            articles_text += f"""### [{i}] {article.title}
- æ¥æº: {article.source_name} | æ—¶é—´: {article.published_at.strftime('%m-%d %H:%M')} | è¡Œä¸š: {article.industry.value}
- å†…å®¹: {content}

"""
        
        task_desc = custom_prompt or f"""
âš ï¸ **é‡è¦æé†’**ï¼šä½ æ”¶åˆ°äº† {article_count} æ¡ä¿¡æ¯ï¼Œä½†**ä¸éœ€è¦é€æ¡åˆ†æ**ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. **æ®‹å¿ç­›é€‰**ï¼šå¿«é€Ÿè¯†åˆ«å‡ºå…¶ä¸­çœŸæ­£å€¼å¾—å…³æ³¨çš„ 20-30% ä¿¡æ¯
2. **ä¸»çº¿èšåˆ**ï¼šå°†é‡è¦ä¿¡æ¯èšåˆæˆ 3-5 æ¡ä¸»çº¿å™äº‹
3. **å¿½ç•¥å™ªéŸ³**ï¼šæ˜ç¡®è¯´æ˜å“ªäº›ä¿¡æ¯è¢«è¿‡æ»¤æ‰äº†ï¼Œä»¥åŠåŸå› 
4. **å†³ç­–å¯¼å‘**ï¼šæ¯ä¸ªåˆ¤æ–­éƒ½è¦æŒ‡å‘"è¯¥åšä»€ä¹ˆ"æˆ–"è¯¥å…³æ³¨ä»€ä¹ˆ"

**ä¸è¦**ï¼š
- âŒ é€æ¡åˆ†ææ¯ç¯‡æ–‡ç« 
- âŒ ç»™æ‰€æœ‰å†…å®¹éƒ½æ‰“ 7/10ã€8/10 çš„åˆ†
- âŒ ç½—åˆ—äº‹ä»¶è€Œä¸åšåˆ¤æ–­
- âŒ å†™è¶…è¿‡ 3 é¡µçš„æŠ¥å‘Šï¼ˆé™¤éä¿¡æ¯å¯†åº¦æé«˜ï¼‰

**è¦åš**ï¼š
- âœ… åªæ·±å…¥åˆ†æçœŸæ­£æ”¹å˜åˆ¤æ–­çš„ä¿¡æ¯
- âœ… å¯¹ä¸é‡è¦çš„ä¿¡æ¯åˆå¹¶æˆ–ä¸€å¥è¯å¸¦è¿‡
- âœ… æ˜ç¡®è¯´"è¿™ç±»ä¿¡æ¯ä»Šå¤©ä¸é‡è¦"
- âœ… æ¯æ®µè¯éƒ½è¦æœ‰"æ‰€ä»¥å‘¢ï¼Ÿ"çš„ç­”æ¡ˆ
"""
        
        return f"""{task_desc}

{articles_text}

---

è¯·æŒ‰ä»¥ä¸‹ç»“æ„ç”Ÿæˆ**é«˜åº¦å‹ç¼©ã€ä¸»çº¿æ¸…æ™°**çš„æŠ¥å‘Šï¼š

# ğŸ“Š è¡Œä¸šæƒ…æŠ¥åˆ†ææŠ¥å‘Š

## ä¸€ã€æ‰§è¡Œæ‘˜è¦ï¼ˆç»™åªè¯»3åˆ†é’Ÿçš„äººï¼‰
ç”¨**3-5æ¡è¦ç‚¹**è¯´æ˜ï¼š
- ä»Šå¤©çœŸæ­£å‘ç”Ÿäº†ä»€ä¹ˆ"ç»“æ„æ€§å˜åŒ–"
- å“ªäº›é£é™©åœ¨ä¸Šå‡ï¼Œå“ªäº›åªæ˜¯å™ªéŸ³
- å“ªäº›æ–¹å‘å€¼å¾—æŒç»­è·Ÿè¸ª

é¿å…ç½—åˆ—äº‹ä»¶ï¼Œå¼ºè°ƒ**åˆ¤æ–­å˜åŒ–**ã€‚

## äºŒã€ä»Šæ—¥ä¸»çº¿å™äº‹ï¼ˆæœ€å¤š5æ¡ï¼‰

### ä¸»çº¿ 1ï¼šã€ä¸€å¥è¯ç»“è®ºå¼æ ‡é¢˜ã€‘
- **æ ¸å¿ƒåˆ¤æ–­**ï¼šè¿™æ¡ä¸»çº¿æ„å‘³ç€ä»€ä¹ˆ
- **å…³é”®ä¿¡å·**ï¼šå“ªäº›äº‹ä»¶æ”¯æ’‘äº†è¿™ä¸ªåˆ¤æ–­ï¼ˆå¼•ç”¨æ–‡ç« ç¼–å·å¦‚[1][5][12]ï¼‰
- **è¢«å¿½ç•¥çš„åè¯**ï¼šæœ‰æ²¡æœ‰ç›¸åä¿¡æ¯ï¼Ÿä¸ºä½•æƒé‡è¾ƒä½
- **å½±å“åŠå¾„**ï¼šå½±å“å“ªäº›å›½å®¶/è¡Œä¸š/èµ„äº§/ç¾¤ä½“

ï¼ˆå…¶ä½™ä¸»çº¿åŒæ ·ç»“æ„ï¼Œæœ€å¤š5æ¡ï¼‰

## ä¸‰ã€å…³é”®ä¿¡å·æ¸…å•

åªåˆ—**çœŸæ­£å€¼å¾—"ç›¯ä½"çš„ä¿¡å·**ï¼Œæ¯ä¸ªä¿¡å·ï¼š
- **ç±»å‹**ï¼šåœ°ç¼˜æ”¿æ²»/äº§ä¸š/æ”¿ç­–/æŠ€æœ¯
- **ä¸ºä½•é‡è¦**ï¼šå®ƒæ”¹å˜äº†ä»€ä¹ˆ"é»˜è®¤å‡è®¾"
- **ç½®ä¿¡åº¦**ï¼šé«˜/ä¸­/ä½
- **è·Ÿè¸ªå»ºè®®**ï¼šæ¥ä¸‹æ¥åº”å…³æ³¨ä»€ä¹ˆ

## å››ã€è¢«è¿‡æ»¤æ‰çš„å†…å®¹

ç®€è¦è¯´æ˜ï¼š
- å“ªå‡ ç±»ä¿¡æ¯ä»Šå¤©å æ¯”å¾ˆé«˜ä½†ä»·å€¼æœ‰é™
- ä¸ºä»€ä¹ˆä¸å€¼å¾—æŠ•å…¥æ³¨æ„åŠ›

## äº”ã€è¡ŒåŠ¨æç¤º

ä»ä»¥ä¸‹è§’åº¦ç»™å‡º**æ˜ç¡®ä½†å…‹åˆ¶**çš„å»ºè®®ï¼š
- **é£é™©è§„é¿**
- **æœºä¼šå¸ƒå±€**
- **ä¿¡æ¯è·Ÿè¸ª**

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}*
*åŸå§‹ä¿¡æ¯æ•°é‡ï¼š{article_count} æ¡*

âš ï¸ **æé†’**ï¼šç›´æ¥è¾“å‡º Markdownï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ã€‚ç¡®ä¿æŠ¥å‘Šå®Œæ•´ï¼Œä¸ä¸­é€”æˆªæ–­ã€‚
"""
    
    def _sync_generate(self, prompt: str):
        """åŒæ­¥è°ƒç”¨ Gemini APIï¼ˆåœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œï¼‰"""
        return self.client.generate_content(prompt)
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        # æ ¹æ®æ¨¡å‹è¿”å›ä¸åŒçš„é…ç½®
        model_configs = {
            'gemini-3-pro-preview': {
                'max_tokens': 1048576,
                'cost_per_1k_tokens': 0.0,
                'description': 'Gemini 3 Pro - æœ€å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£æ¨¡å‹'
            },
            'gemini-3-flash-preview': {
                'max_tokens': 1048576,
                'cost_per_1k_tokens': 0.0,
                'description': 'Gemini 3 Flash - æœ€å‡è¡¡çš„æ¨¡å‹ï¼Œé€Ÿåº¦ä¸æ™ºèƒ½å…¼é¡¾'
            },
            'gemini-2.5-flash': {
                'max_tokens': 1048576,
                'cost_per_1k_tokens': 0.0,
                'description': 'Gemini 2.5 Flash - æ€§ä»·æ¯”æœ€ä½³ï¼ˆç¨³å®šç‰ˆï¼‰'
            },
            'gemini-2.5-pro': {
                'max_tokens': 1048576,
                'cost_per_1k_tokens': 0.0,
                'description': 'Gemini 2.5 Pro - æ›´å¼ºå¤§çš„æ¨ç†èƒ½åŠ›'
            },
            'gemini-2.0-flash': {
                'max_tokens': 1048576,
                'cost_per_1k_tokens': 0.0,
                'description': 'Gemini 2.0-Flash - å°†äº2026å¹´3æœˆ31æ—¥å¼ƒç”¨'
            }
        }
        
        config = model_configs.get(self.model, model_configs['gemini-2.5-flash'])
        
        return {
            'backend': 'gemini',
            'model': self.model,
            'max_tokens': config['max_tokens'],
            'cost_per_1k_tokens': config['cost_per_1k_tokens'],
            'description': config['description']
        }
