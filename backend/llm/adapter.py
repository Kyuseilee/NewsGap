"""
LLM é€‚é…å™¨åŸºç±»ä¸å·¥å‚

å®šä¹‰ç»Ÿä¸€çš„ LLM æ¥å£ï¼Œæ”¯æŒå¤šç§åç«¯
"""

import tiktoken
from typing import List, Optional
from abc import ABC, abstractmethod

from models import (
    Article, Analysis, AnalysisType,
    LLMAdapterInterface, Trend, Signal, InformationGap
)


class BaseLLMAdapter(LLMAdapterInterface, ABC):
    """LLM é€‚é…å™¨åŸºç±»"""
    
    def __init__(self, api_key: Optional[str] = None, model: Optional[str] = None):
        self.api_key = api_key
        self.model = model
    
    def estimate_cost(self, articles: List[Article]) -> dict:
        """ä¼°ç®—åˆ†ææˆæœ¬"""
        # ä¼°ç®—æ€» token æ•°
        total_tokens = self._estimate_tokens(articles)
        
        model_info = self.get_model_info()
        cost_per_1k = model_info.get('cost_per_1k_tokens', 0)
        
        estimated_cost = (total_tokens / 1000) * cost_per_1k
        
        return {
            'token_count': total_tokens,
            'estimated_cost_usd': estimated_cost,
            'model': model_info['model']
        }
    
    def _estimate_tokens(self, articles: List[Article]) -> int:
        """ä¼°ç®— token æ•°é‡"""
        # ç®€å•ä¼°ç®—ï¼šæ–‡ç« å†…å®¹ + ç³»ç»Ÿæç¤º
        total_text = ""
        for article in articles:
            total_text += f"{article.title}\n{article.content}\n\n"
        
        # åŠ ä¸Šç³»ç»Ÿæç¤ºè¯ï¼ˆçº¦ 500 tokensï¼‰
        system_prompt_tokens = 500
        
        # ä½¿ç”¨ tiktoken ä¼°ç®—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        try:
            encoding = tiktoken.get_encoding("cl100k_base")
            content_tokens = len(encoding.encode(total_text))
            return content_tokens + system_prompt_tokens
        except Exception:
            # ç®€å•ä¼°ç®—ï¼šä¸­æ–‡çº¦ 1.5 å­—/tokenï¼Œè‹±æ–‡çº¦ 0.25 è¯/token
            char_count = len(total_text)
            estimated_tokens = int(char_count / 1.5)
            return estimated_tokens + system_prompt_tokens
    
    def _build_system_prompt(self, analysis_type: AnalysisType) -> str:
        """æ„å»ºç³»ç»Ÿæç¤ºè¯ - ä¸¤é˜¶æ®µåˆ†ææ³•"""
        
        # ç¬¬ä¸€é˜¶æ®µï¼šç­›é€‰å’Œä¸»çº¿è¯†åˆ«
        stage1_prompt = """ä½ ä¸æ˜¯æ–°é—»æ‘˜è¦å™¨ï¼Œè€Œæ˜¯ä¸€å"å†³ç­–å¯¼å‘å‹è¡Œä¸šæƒ…æŠ¥åˆ†æå¸ˆ"ã€‚

ä½ çš„ç›®æ ‡ä¸æ˜¯è¦†ç›–æ‰€æœ‰ä¿¡æ¯ï¼Œè€Œæ˜¯ï¼š
- åœ¨å¤§é‡æ‚è®¯ä¸­ï¼Œå¿«é€Ÿè¯†åˆ«**çœŸæ­£æ”¹å˜æ ¼å±€çš„å°‘æ•°ä¿¡å·**
- ä¸º"ç†æ€§å†³ç­–è€…"æä¾›**å¯è¡ŒåŠ¨ã€å¯å–èˆã€å¯å¿½ç•¥çš„ä¿¡æ¯ç»“æ„**

ã€æ ¸å¿ƒåŸåˆ™ã€‘
1. **æ®‹å¿ç­›é€‰åŸåˆ™**ï¼šå…è®¸å¿½ç•¥ã€åˆå¹¶ã€å¼±åŒ–å¤§é‡ä½ä»·å€¼æ–‡ç« ï¼Œåªæœ‰"æ”¹å˜åˆ¤æ–­"çš„å†…å®¹æ‰å€¼å¾—å±•å¼€
2. **ä¸»çº¿ä¼˜å…ˆåŸåˆ™**ï¼šå…ˆè¯†åˆ«3-5æ¡"ä»Šæ—¥ä¸»çº¿å™äº‹"ï¼Œæ‰€æœ‰æ–‡ç« åªæ˜¯è¿™äº›ä¸»çº¿çš„"è¯æ®"æˆ–"å™ªéŸ³"
3. **å»å‡å€¼åŸåˆ™**ï¼šé¿å…å¤§é‡7/10ã€8/10çš„æ¨¡ç³Šè¯„åˆ†ï¼Œé‡è¦æ€§å¿…é¡»å½¢æˆæ˜æ˜¾æ¢¯åº¦ï¼ˆ10/8/5/å¿½ç•¥ï¼‰
4. **å†³ç­–è§†è§’åŸåˆ™**ï¼šå‡è®¾è¯»è€…å…³æ³¨å®è§‚é£é™©ã€äº§ä¸šæ–¹å‘ä¸ä¸­é•¿æœŸé…ç½®çš„ç†æ€§å†³ç­–è€…
5. **å‹ç¼©ä¼˜å…ˆåŸåˆ™**ï¼šå®å¯å°‘å†™ä¸€åŠï¼Œä¹Ÿä¸è¦ä¿¡æ¯å¯†åº¦ä¸‹é™

ã€å…è®¸çš„æ“ä½œã€‘
- å°†åŒç±»æ–‡ç« åˆå¹¶ä¸º"ä¿¡æ¯ç°‡"
- å¯¹ä½ä»·å€¼å†…å®¹åªåšä¸€å¥è¯å¤„ç†ï¼Œç”šè‡³å®Œå…¨ä¸å†™
- æ˜ç¡®æŒ‡å‡ºï¼š"è¿™ä¸€ç±»ä¿¡æ¯ä»Šå¤©ä¸é‡è¦"
"""
        
        if analysis_type == AnalysisType.COMPREHENSIVE:
            return stage1_prompt + """
è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡º**é«˜åº¦å‹ç¼©ã€ä¸»çº¿æ¸…æ™°**çš„ Markdown æŠ¥å‘Šï¼š

# ğŸ“Š è¡Œä¸šæƒ…æŠ¥åˆ†ææŠ¥å‘Š

## ä¸€ã€æ‰§è¡Œæ‘˜è¦ï¼ˆç»™åªè¯»3åˆ†é’Ÿçš„äººï¼‰
ç”¨**3-5æ¡è¦ç‚¹**è¯´æ˜ï¼š
- ä»Šå¤©çœŸæ­£å‘ç”Ÿäº†ä»€ä¹ˆ"ç»“æ„æ€§å˜åŒ–"
- å“ªäº›é£é™©åœ¨ä¸Šå‡ï¼Œå“ªäº›åªæ˜¯å™ªéŸ³
- å“ªäº›æ–¹å‘å€¼å¾—æŒç»­è·Ÿè¸ª

é¿å…ç½—åˆ—äº‹ä»¶ï¼Œå¼ºè°ƒ**åˆ¤æ–­å˜åŒ–**ã€‚

---

## äºŒã€ä»Šæ—¥ä¸»çº¿å™äº‹ï¼ˆæœ€å¤š5æ¡ï¼‰

### ä¸»çº¿ 1ï¼šã€ä¸€å¥è¯ç»“è®ºå¼æ ‡é¢˜ã€‘
- **æ ¸å¿ƒåˆ¤æ–­**ï¼šè¿™æ¡ä¸»çº¿æ„å‘³ç€ä»€ä¹ˆ
- **å…³é”®ä¿¡å·**ï¼šå“ªäº›äº‹ä»¶æ”¯æ’‘äº†è¿™ä¸ªåˆ¤æ–­ï¼ˆå¼•ç”¨æ–‡ç« æ ‡é¢˜ï¼‰
- **è¢«å¿½ç•¥çš„åè¯**ï¼šæœ‰æ²¡æœ‰ç›¸åä¿¡æ¯ï¼Ÿä¸ºä½•æƒé‡è¾ƒä½
- **å½±å“åŠå¾„**ï¼šå½±å“å“ªäº›å›½å®¶/è¡Œä¸š/èµ„äº§/ç¾¤ä½“

ï¼ˆå…¶ä½™ä¸»çº¿åŒæ ·ç»“æ„ï¼Œæœ€å¤š5æ¡ï¼‰

---

## ä¸‰ã€å…³é”®ä¿¡å·æ¸…å•ï¼ˆåªåˆ—çœŸæ­£å€¼å¾—"ç›¯ä½"çš„ä¿¡å·ï¼‰

### ä¿¡å· Xï¼šã€æ˜ç¡®ã€å…·ä½“ã€å¯éªŒè¯ã€‘
- **ç±»å‹**ï¼šåœ°ç¼˜æ”¿æ²»/äº§ä¸š/æ”¿ç­–/æŠ€æœ¯
- **ä¸ºä½•é‡è¦**ï¼šå®ƒæ”¹å˜äº†ä»€ä¹ˆ"é»˜è®¤å‡è®¾"
- **ç½®ä¿¡åº¦**ï¼šé«˜/ä¸­/ä½
- **è·Ÿè¸ªå»ºè®®**ï¼šæ¥ä¸‹æ¥åº”å…³æ³¨ä»€ä¹ˆå˜åŒ–

---

## å››ã€è¢«è¿‡æ»¤æ‰çš„å†…å®¹

ç®€è¦è¯´æ˜ï¼š
- å“ªå‡ ç±»ä¿¡æ¯ä»Šå¤©**å æ¯”å¾ˆé«˜ä½†ä»·å€¼æœ‰é™**
- ä¸ºä»€ä¹ˆä¸å€¼å¾—æŠ•å…¥æ³¨æ„åŠ›ï¼ˆä¾‹å¦‚ï¼šé‡å¤ã€è±¡å¾æ€§ã€æƒ…ç»ªæ€§ï¼‰

---

## äº”ã€è¡ŒåŠ¨æç¤ºï¼ˆä¸æ˜¯é¢„æµ‹ï¼Œæ˜¯åº”å¯¹ï¼‰

åˆ†åˆ«ä»ä»¥ä¸‹è§’åº¦ç»™å‡º**æ˜ç¡®ä½†å…‹åˆ¶**çš„å»ºè®®ï¼š
- **é£é™©è§„é¿**
- **æœºä¼šå¸ƒå±€**
- **ä¿¡æ¯è·Ÿè¸ª**

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{{time}}*
*åŸå§‹ä¿¡æ¯æ•°é‡ï¼š{{total}}*
*è¿›å…¥åˆ†ææ ¸å¿ƒçš„ä¿¡æ¯æ¯”ä¾‹ï¼šçº¦ {{ratio}}%*
"""
        
        else:
            return stage1_prompt + """
è¯·ç”Ÿæˆç®€æ´çš„æƒ…æŠ¥ç®€æŠ¥ï¼ŒåŒ…å«ï¼š
- æ‰§è¡Œæ‘˜è¦ï¼ˆ3-5æ¡è¦ç‚¹ï¼‰
- å…³é”®ä¸»çº¿ï¼ˆ2-3æ¡ï¼‰
- è¡ŒåŠ¨å»ºè®®
"""
    
    def _build_user_prompt(
        self,
        articles: List[Article],
        analysis_type: AnalysisType,
        custom_prompt: Optional[str] = None
    ) -> str:
        """æ„å»ºç”¨æˆ·æç¤ºè¯ - æ™ºèƒ½å‹ç¼©ç‰ˆ"""
        
        # æ ¹æ®æ–‡ç« æ•°é‡åŠ¨æ€è°ƒæ•´å†…å®¹é•¿åº¦
        article_count = len(articles)
        if article_count <= 20:
            max_content_length = 1000  # å…è®¸è¾ƒé•¿å†…å®¹
        elif article_count <= 50:
            max_content_length = 600   # ä¸­ç­‰å‹ç¼©
        elif article_count <= 100:
            max_content_length = 400   # é«˜åº¦å‹ç¼©
        else:
            max_content_length = 300   # æåº¦å‹ç¼©
        
        # æ„å»ºæ–‡ç« åˆ—è¡¨ï¼ˆå‹ç¼©ç‰ˆï¼‰
        articles_text = f"# å¾…åˆ†æä¿¡æ¯æºï¼ˆå…± {article_count} æ¡ï¼‰\n\n"
        
        for i, article in enumerate(articles, 1):
            # å‹ç¼©å†…å®¹
            content = article.content[:max_content_length]
            if len(article.content) > max_content_length:
                content += "..."
            
            articles_text += f"""### [{i}] {article.title}
- æ¥æº: {article.source_name} | æ—¶é—´: {article.published_at.strftime('%Y-%m-%d %H:%M')} | è¡Œä¸š: {article.industry.value}
- å†…å®¹: {content}

"""
        
        # åˆ†ææŒ‡ä»¤
        if custom_prompt:
            task_description = custom_prompt
        else:
            if analysis_type == AnalysisType.COMPREHENSIVE:
                task_description = f"""
âš ï¸ **é‡è¦æé†’**ï¼šä½ æ”¶åˆ°äº† {article_count} æ¡ä¿¡æ¯ï¼Œä½†**ä¸éœ€è¦é€æ¡åˆ†æ**ã€‚

ä½ çš„ä»»åŠ¡æ˜¯ï¼š
1. **æ®‹å¿ç­›é€‰**ï¼šå¿«é€Ÿè¯†åˆ«å‡ºå…¶ä¸­çœŸæ­£å€¼å¾—å…³æ³¨çš„ 20-30% ä¿¡æ¯
2. **ä¸»çº¿èšåˆ**ï¼šå°†é‡è¦ä¿¡æ¯èšåˆæˆ 3-5 æ¡ä¸»çº¿å™äº‹
3. **å¿½ç•¥å™ªéŸ³**ï¼šæ˜ç¡®è¯´æ˜å“ªäº›ä¿¡æ¯è¢«è¿‡æ»¤æ‰äº†ï¼Œä»¥åŠåŸå› 
4. **å†³ç­–å¯¼å‘**ï¼šæ¯ä¸ªåˆ¤æ–­éƒ½è¦æŒ‡å‘"è¯¥åšä»€ä¹ˆ"æˆ–"è¯¥å…³æ³¨ä»€ä¹ˆ"

**ä¸è¦**ï¼š
- âŒ é€æ¡åˆ†ææ¯ç¯‡æ–‡ç« 
- âŒ ç»™æ‰€æœ‰å†…å®¹éƒ½æ‰“ 7/10ã€8/10 çš„åˆ†
- âŒ ç½—åˆ—äº‹ä»¶è€Œä¸åšåˆ¤æ–­
- âŒ å†™è¶…è¿‡ 3 é¡µçš„æŠ¥å‘Šï¼ˆé™¤éä¿¡æ¯å¯†åº¦æé«˜ï¼‰

**è¦åš**ï¼š
- âœ… åªæ·±å…¥åˆ†æçœŸæ­£æ”¹å˜åˆ¤æ–­çš„ä¿¡æ¯
- âœ… å¯¹ä¸é‡è¦çš„ä¿¡æ¯åˆå¹¶æˆ–ä¸€å¥è¯å¸¦è¿‡
- âœ… æ˜ç¡®è¯´"è¿™ç±»ä¿¡æ¯ä»Šå¤©ä¸é‡è¦"
- âœ… æ¯æ®µè¯éƒ½è¦æœ‰"æ‰€ä»¥å‘¢ï¼Ÿ"çš„ç­”æ¡ˆ
"""
            else:
                task_description = "è¯·ç”Ÿæˆç®€æ´çš„æƒ…æŠ¥æ‘˜è¦ã€‚"
        
        return f"""{task_description}

{articles_text}

---
**è¾“å‡ºæ ¼å¼**ï¼šç›´æ¥è¾“å‡º Markdown æ ¼å¼çš„æŠ¥å‘Šï¼Œä¸è¦ç”¨ä»£ç å—åŒ…è£¹ã€‚
**è´¨é‡æ ‡å‡†**ï¼šä¿¡æ¯å¯†åº¦ > è¦†ç›–ç‡ï¼Œåˆ¤æ–­æ¸…æ™° > é¢é¢ä¿±åˆ°ã€‚
"""
    
    @abstractmethod
    async def analyze(
        self,
        articles: List[Article],
        analysis_type: AnalysisType,
        custom_prompt: Optional[str] = None
    ) -> Analysis:
        """å­ç±»å®ç°å…·ä½“çš„åˆ†æé€»è¾‘"""
        pass
    
    @abstractmethod
    def get_model_info(self) -> dict:
        """å­ç±»æä¾›æ¨¡å‹ä¿¡æ¯"""
        pass


# ============================================================================
# LLM é€‚é…å™¨å·¥å‚
# ============================================================================

def create_llm_adapter(
    backend: str,
    api_key: Optional[str] = None,
    model: Optional[str] = None
) -> BaseLLMAdapter:
    """
    åˆ›å»º LLM é€‚é…å™¨
    
    Args:
        backend: "ollama", "openai", "deepseek", "gemini"
        api_key: API å¯†é’¥ï¼ˆæœ¬åœ°æ¨¡å‹ä¸éœ€è¦ï¼‰
        model: æ¨¡å‹åç§°
    """
    backend = backend.lower()
    
    if backend == "ollama":
        from llm.ollama_adapter import OllamaAdapter
        return OllamaAdapter(model=model)
    
    elif backend == "openai":
        from llm.openai_adapter import OpenAIAdapter
        return OpenAIAdapter(api_key=api_key, model=model)
    
    elif backend == "deepseek":
        from llm.deepseek_adapter import DeepSeekAdapter
        return DeepSeekAdapter(api_key=api_key, model=model)
    
    elif backend == "gemini":
        from llm.gemini_adapter import GeminiAdapter
        return GeminiAdapter(api_key=api_key, model=model)
    
    else:
        raise ValueError(f"Unknown LLM backend: {backend}")
